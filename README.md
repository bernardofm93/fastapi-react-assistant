# ğŸ§  FastAPI ReAct Assistant
![Python Version](https://img.shields.io/badge/Python-3.10%2B-blue)
![FastAPI Version](https://img.shields.io/badge/FastAPI-0.111.1-green)
![LangChain](https://img.shields.io/badge/LangChain-0.1.20-purple)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-orange)

Assistente Conversacional que utiliza **ReAct Agent** para responder perguntas sobre produtos e SAC, combinando **consulta em SQL** e/ou **busca semÃ¢ntica**.  
O projeto inclui:

âœ… **FastAPI** como backend HTTP  
âœ… **LangGraph + LangChain** para criaÃ§Ã£o do agente ReAct  
âœ… **Ollama** como servidor local de modelos LLM e embeddings  
âœ… **ChromaDB** como banco vetorial do SAC e dos nomes dos produtos  
âœ… **SQLite** para persistir histÃ³rico de conversas e catÃ¡logo de produtos  

---

## ğŸ–¥ï¸ :computer: Requisitos

- :snake: [**Python 3.10+**](https://www.python.org/downloads/)
- :whale: [**Docker**](https://www.docker.com/products/docker-desktop/)
- [**Ollama**](https://ollama.com/download) (Para rodar modelos de LLM e embeddings)


â¸»

ğŸƒ :pushpin: Rodando localmente

Antes de tudo, certifique-se de que Ollama estÃ¡ rodando e de que os modelos foram baixados.

ğŸ“¥ Clone o repositÃ³rio

git clone https://github.com/seu-usuario/fastapi-react-assistant.git


â¸»

ğŸ§° Instale as dependÃªncias

python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt


â¸»

ğŸ¤– Baixe modelos no Ollama

ollama pull llama3.1
ollama pull mxbai-embed-large


â¸»

ğŸ—ƒï¸ Preprocessamento inicial

Gera o banco de produtos (produtos.db) e popula o ChromaDB com embeddings:

python -m app.db.preprocess


â¸»

ğŸš€ Suba a API

uvicorn app.main:app --reload

Por padrÃ£o, estarÃ¡ disponÃ­vel em:

http://localhost:8000


â¸»

ğŸ“š Endpoints

ğŸ”¹ POST /chat/

Envia uma pergunta e recebe a resposta do assistente.

Exemplo de body:

{
  "question": "Quais calÃ§as custam menos de 200 reais?",
  "thread_id": "minha-conversa-123"
}


â¸»

ğŸ”¹ GET /historico/{thread_id}

Recupera todo o histÃ³rico da conversa.

ParÃ¢metros de query opcionais:
	â€¢	columns: Quais colunas retornar (role, content, timestamp, tokens_input, tokens_output)

Exemplo:

/historico/minha-conversa-123?columns=role&columns=content


â¸»

ğŸ”¹ Swagger Docs

Interface interativa:

http://localhost:8000/docs


â¸»

ğŸ³ :whale: Docker

VocÃª tambÃ©m pode rodar tudo via Docker Compose.

â¸»

ğŸ› ï¸ Build da imagem

docker build -t fastapi-react-assistant .


â¸»

ğŸš€ Run com Docker

Certifique-se que o Ollama estÃ¡ rodando no host e passe o .env:

docker run -d --env-file .env -p 8000:8000 --name assistant fastapi-react-assistant


â¸»


ğŸ§  Exemplos de uso com cURL

Enviar pergunta:

curl -X POST "http://localhost:8000/chat/" \
  -H "Content-Type: application/json" \
  -d '{"question":"Quantos produtos custam abaixo de 200 reais?","thread_id":"teste"}'

Consultar histÃ³rico:

curl "http://localhost:8000/historico/teste


â¸»

âš ï¸ ObservaÃ§Ãµes
	â€¢	O Ollama precisa estar rodando localmente ou acessÃ­vel no OLLAMA_BASE_URL.
	â€¢	Os modelos devem estar baixados previamente.