# ğŸ§  FastAPI ReAct Assistant
![Python Version](https://img.shields.io/badge/Python-3.10%2B-blue)
![FastAPI Version](https://img.shields.io/badge/FastAPI-0.115.13-green)
![LangChain](https://img.shields.io/badge/LangChain-0.3.26-purple)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-orange)

Assistente Conversacional que utiliza um **Agente ReAct** para responder perguntas sobre produtos e SAC, combinando **consulta em SQL** e/ou **busca semÃ¢ntica**.  
O projeto inclui:

âœ… **FastAPI** como backend HTTP  
âœ… **LangGraph + LangChain** para criaÃ§Ã£o do agente ReAct  
âœ… **Ollama** como servidor local de modelos LLM e embeddings  
âœ… **ChromaDB** como banco vetorial do SAC e dos nomes dos produtos  
âœ… **SQLite** para persistir histÃ³rico de conversas e catÃ¡logo de produtos

---

ğŸ§© VisÃ£o Geral

O projeto implementa um ReAct Agent que combina raciocÃ­nio e aÃ§Ãµes sobre diferentes fontes de dados para responder perguntas relacionadas ao catÃ¡logo e ao serviÃ§o de atendimento ao cliente da C&A.

â¸»

ğŸ–¼ï¸ Arquitetura do Agente

A arquitetura de um agente ReAct pode ser representada por um grafo, contendo um nodo de assistente e outro de ferramentas, conforme imagem abaixo. 

![alt text](image.png)

O agente ReAct segue a lÃ³gica:
	1.	Recebe a pergunta do usuÃ¡rio
	2.	Analisa o contexto e o histÃ³rico
	3.	Decide qual ou quais ferramentas utilizar
	4.	Executa buscas SQL ou semÃ¢nticas conforme necessÃ¡rio
	5.	Combina e organiza a resposta final


â¸»

ğŸ› ï¸ Ferramentas Utilizadas

O agente conta com duas ferramentas principais:

- Busca SQL de Produtos	Realiza consultas estruturadas na base de dados SQLite para retornar informaÃ§Ãµes detalhadas sobre produtos, como preÃ§o, tÃ­tulo e descriÃ§Ã£o. Ideal para perguntas objetivas que envolvem filtros e atributos conhecidos.
- Busca SemÃ¢ntica de FAQ	Utiliza embeddings gerados via Ollama e ChromaDB para recuperar respostas por similaridade semÃ¢ntica. Indicada para perguntas abertas ou quando o usuÃ¡rio utiliza linguagem natural sem correspondÃªncia direta no catÃ¡logo.

â¸»


## ğŸ–¥ï¸ :computer: Requisitos

- :snake: [**Python 3.10+**](https://www.python.org/downloads/)
- :whale: [**Docker**](https://www.docker.com/products/docker-desktop/)
- [**Ollama**](https://ollama.com/download) (Para rodar modelos de LLM e embeddings)


â¸»

ğŸƒ :pushpin: Rodando localmente

Antes de tudo, certifique-se de que o Ollama estÃ¡ rodando e de que os modelos foram baixados.


ğŸ“¥ Clone o repositÃ³rio

        git clone https://github.com/seu-usuario/fastapi-react-assistant.git


â¸»

ğŸ§° Instale as dependÃªncias

        python -m venv .venv
        source .venv/bin/activate
        pip install -r requirements.txt


â¸»

ğŸ¤– Baixe modelos no Ollama

        ollama pull llama3.1
        ollama pull mxbai-embed-large


â¸»

ğŸ—ƒï¸ Preprocessamento inicial

Gera o banco de produtos (produtos.db) e popula o ChromaDB com embeddings:

        python -m app.db.preprocess


â¸»

ğŸš€ Suba a API

        uvicorn app.main:app --reload

Por padrÃ£o, estarÃ¡ disponÃ­vel em:

http://localhost:8000


â¸»

ğŸ“š Endpoints

ğŸ”¹ POST /chat/

Envia uma pergunta e recebe a resposta do assistente.

Exemplo de Request body:

        {
        "question": "Quais calÃ§as custam menos de 200 reais?",
        "thread_id": "thread-123"
        }


â¸»

ğŸ”¹ GET /historico/{thread_id}

Recupera todo o histÃ³rico da conversa a partir do ID da thread.


â¸»

ğŸ”¹ Swagger Docs

Interface interativa:

http://localhost:8000/docs


â¸»

ğŸ³ :whale: Docker

Rodando via Docker Compose.

â¸»

ğŸ› ï¸ Build da imagem

        docker build -t fastapi-react-assistant .


â¸»

ğŸš€ Run com Docker

Certifique-se que o Ollama estÃ¡ rodando no host e passe o .env:

        docker run -d --env-file .env -p 8000:8000 --name assistant fastapi-react-assistant


â¸»


ğŸ§  Exemplos de uso com cURL

Enviar pergunta:

        curl -X POST "http://localhost:8000/chat/" \
        -H "Content-Type: application/json" \
        -d '{"question":"Quantos produtos custam abaixo de 200 reais?","thread_id":"teste"}'

Consultar histÃ³rico:

        curl "http://localhost:8000/historico/teste


â¸»

ğŸ““ ğŸ§ª Testando em Jupyter Notebook

Caso seja mais acessÃ­vel, Ã© possÃ­vel realizar o teste no arquivo **teste_agente.ipynb**. 