{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8097a2e7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0182fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from uuid import uuid4\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import chromadb\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "client = chromadb.Client()\n",
    "embeddings = OllamaEmbeddings(model=\"granite-embedding:278m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfacd27",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bed7b6",
   "metadata": {},
   "source": [
    "### Criação da base em SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50512df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/dados-produtos.json\")\n",
    "conn = sqlite3.connect(\"produtos.db\", check_same_thread=False)\n",
    "df.to_sql(\"produtos\", conn, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ecce1",
   "metadata": {},
   "source": [
    "### Criação do banco vetorial com nomes dos produtos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7688c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_collection = client.create_collection(name=\"catalog\")\n",
    "for i, row in df.iterrows():\n",
    "    catalog_collection.add(\n",
    "        ids=[str(uuid4())],\n",
    "        embeddings=embeddings.embed_query(row['description']),\n",
    "        documents=[row['description']],\n",
    "        metadatas=[{\"productId\": row['productId'], \"title\": row['title']}]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"calça de academia pra mulher\"\n",
    "\n",
    "results = catalog_collection.query(\n",
    "  query_embeddings=embeddings.embed_query(input_query),\n",
    "  n_results=5,\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d281baf7",
   "metadata": {},
   "source": [
    "### Criação do banco vetorial do SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(name=\"sac\")\n",
    "\n",
    "with open('data/dados-sac.md', 'r', encoding='utf-8') as f:\n",
    "    sac = f.read()\n",
    "\n",
    "def split_faq_by_question_and_sentences(text):\n",
    "    pattern = r\"# (.*\\?)\"\n",
    "    matches = list(re.finditer(pattern, text))\n",
    "    chunks = []\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "        question = match.group(1).strip()\n",
    "        answer_block = text[start:end].strip()\n",
    "\n",
    "        subchunks = re.split(r\"\\.\\s*\\n\", answer_block)\n",
    "        subchunks = [chunk.strip() for chunk in subchunks if chunk.strip()]\n",
    "\n",
    "        for j, sub in enumerate(subchunks):\n",
    "            chunk_text = f\"Pergunta: {question}\\nResposta: {sub}.\"\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": {\n",
    "                    \"question\": question,\n",
    "                    \"chunk_id\": f\"{i}-{j}\"\n",
    "                }\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "perguntas_chunks = split_faq_by_question_and_sentences(sac)\n",
    "perguntas_chunks = [i['text'] for i in perguntas_chunks]\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(perguntas_chunks))]\n",
    "for i, d in enumerate(perguntas_chunks):\n",
    "    collection.add(\n",
    "        ids=[uuids[i]],\n",
    "        embeddings=embeddings.embed_query(d),\n",
    "        documents=[d]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a4f647",
   "metadata": {},
   "source": [
    "## Criação das tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05392d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sql(query):\n",
    "    \"\"\"Busca informações de produtos, suas características e preços no banco via SQLite. Considere o seguinte schema e uma pequena amostra da tabela 'produtos':\n",
    "    [(0, 'index', 'BIGINT', 0, None, 0), (1, 'productId', 'BIGINT', 0, None, 0), (2, 'title', 'TEXT', 0, None, 0), (3, 'price', 'FLOAT', 0, None, 0), (4, 'image', 'TEXT', 0, None, 0), (5, 'description', 'TEXT', 0, None, 0)]\n",
    "\n",
    "    [(1666, 2009094, 'camisa manga longa com bolso branco', 139.99, 'https://cea.vteximg.com.br/arquivos/ids/57197111/camisa-manga-longa-com-bolso-branco-7591834-Branco_1.jpg?v=638128914633000000', 'camisa manga longa bolso branco moda masculina roupas blusas 1 2 3 4 5 6 7'), (1667, 2017098, 'camisa manga curta com bolso branca', 119.99, 'https://cea.vteximg.com.br/arquivos/ids/53195213/camisa-manga-curta-com-bolso-branco-7602490-Branco_1.jpg?v=637801158434200000', 'camisa manga curta bolso branca moda masculina roupas blusas branco 3 4 5 2 1 6 7'), (1326, 2062137, 'Calça Jeans Feminina Super Skinny com Zíper na Barra Azul Médio', 119.99, 'https://cea.vteximg.com.br/arquivos/ids/52448505/Calca-Jeans-Feminina-Super-Skinny-com-Ziper-na-Barra-Azul-Medio-7936103-Azul_Medio_1.jpg?v=637774164382300000', 'calca jeans feminina super skinny ziper barra azul medio moda roupas calcas 40 46 36 38 44 42 48 34')]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_sql_query(query, conn).to_string()\n",
    "    except Exception as e:\n",
    "        return f\"Erro na consulta SQL: {str(e)}\"\n",
    "    \n",
    "def search_sac(question):\n",
    "    \"\"\"Busca informações sobre o serviço de atendimento ao cliente.\"\"\"\n",
    "    embedded = embeddings.embed_query(question)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[embedded],\n",
    "        n_results=5\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def search_proper_nouns_catalog(question):\n",
    "    \"\"\"\n",
    "    Use para filtrar a coluna 'description' na tabela 'produtos'. \n",
    "    O input pode ser a descrição do que o usuário precisa ou nome aproximado do produto, o output é o nome exato como consta na coluna 'description'.\n",
    "    \"\"\"\n",
    "    embedded = embeddings.embed_query(question)\n",
    "    results = catalog_collection.query(\n",
    "        query_embeddings=[embedded],\n",
    "        n_results=5\n",
    "    )\n",
    "    print(results['documents'][0])\n",
    "    print(results['metadatas'][0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e540a",
   "metadata": {},
   "source": [
    "## Criação do agente e configuração do modelo de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model=\"llama3.1\",\n",
    "    model_provider=\"ollama\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "REACT_PROMPT = \"\"\"\n",
    "Você é um assistente especializado em responder perguntas sobre a C&A.\n",
    "\n",
    "Você tem acesso a duas fontes principais de informação:\n",
    "- As respostas para as perguntas mais frequentes dos clientes.\n",
    "- O catálogo de produtos, incluindo descrições detalhadas e preços.\n",
    "\n",
    "<instructions>\n",
    "Ao receber uma pergunta do usuário:\n",
    "\t1.\tConsidere sempre o contexto da conversa antes de decidir qual ferramenta utilizar.\n",
    "\t2.\tAvalie cuidadosamente se a informação encontrada é relevante e suficiente para responder com precisão.\n",
    "\t3.\tCaso necessário, utilize mais de uma ferramenta para formular a resposta.\n",
    "\t4.\tOrganize suas respostas de forma clara, direta e útil para o usuário.\n",
    "</instructions>\n",
    "    \n",
    "Pense passo a passo antes de responder, garantindo que sua resposta seja completa e alinhada à pergunta feita.\n",
    "\"\"\"\n",
    "\n",
    "agent_executor = create_react_agent(model=model, \n",
    "                                    tools=[search_sac, search_sql, search_proper_nouns_catalog], \n",
    "                                    prompt=REACT_PROMPT, \n",
    "                                    checkpointer=checkpointer)\n",
    "\n",
    "def answer_question(question: str, thread_id: str = \"\"):\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    last_event = None\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "\n",
    "    for event in agent_executor.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
    "        stream_mode=\"values\", config=config,\n",
    "    ):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "        last_event = event            \n",
    "\n",
    "    for message in last_event[\"messages\"]:\n",
    "        if isinstance(message, AIMessage):\n",
    "            input_tokens += message.usage_metadata['input_tokens']\n",
    "            output_tokens += message.usage_metadata['output_tokens']\n",
    "    \n",
    "    resposta = last_event[\"messages\"][-1].content\n",
    "\n",
    "    return {\n",
    "        \"query\": question,\n",
    "        \"answer\": resposta,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0ce7f",
   "metadata": {},
   "source": [
    "## Teste\n",
    "\n",
    "Para testar a continuidade da conversa, basta repetir a função answer_question com o mesmo thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2917572",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"Olá!\", thread_id=\"teste-123\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
